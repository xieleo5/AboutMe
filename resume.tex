\documentclass[letterpaper,11pt]{article}

\usepackage{latexsym}
\usepackage[empty]{fullpage}
\usepackage{titlesec}
\usepackage{marvosym}
\usepackage[usenames,dvipsnames]{color}
\usepackage{verbatim}
\usepackage{enumitem}
\usepackage[colorlinks=true, linkcolor=blue]{hyperref}
\usepackage{fancyhdr}
\usepackage[english]{babel}
\usepackage{tabularx}
\usepackage{fontawesome5}
\usepackage{multicol}
\setlength{\multicolsep}{-3.0pt}
\setlength{\columnsep}{-1pt}
\input{glyphtounicode}


%----------FONT OPTIONS----------
% sans-serif
% \usepackage[sfdefault]{FiraSans}
% \usepackage[sfdefault]{roboto}
% \usepackage[sfdefault]{noto-sans}
% \usepackage[default]{sourcesanspro}

% serif
\usepackage{CormorantGaramond}
\usepackage{charter}


\pagestyle{fancy}
\fancyhf{} % clear all header and footer fields
\fancyfoot{}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}

% Adjust margins
\addtolength{\oddsidemargin}{-0.6in}
\addtolength{\evensidemargin}{-0.5in}
\addtolength{\textwidth}{1.19in}
\addtolength{\topmargin}{-.7in}
\addtolength{\textheight}{1.4in}

\urlstyle{same}

\raggedbottom
\raggedright
\setlength{\tabcolsep}{0in}

% Sections formatting
\titleformat{\section}{
  \vspace{-8pt}\raggedright\Large\bfseries
}{}{0em}{}[\color{black}\titlerule \vspace{-5pt}]

% Ensure that generate pdf is machine readable/ATS parsable
\pdfgentounicode=1

%-------------------------
% Custom commands
\newcommand{\resumeItem}[1]{
  \item\small{
    {\raggedright #1 \vspace{-2pt}}
  }
}

\newcommand{\classesList}[4]{
    \item\small{
        {#1 #2 #3 #4 \vspace{-2pt}}
  }
}

\newcommand{\resumeSubheading}[5]{
  \vspace{-1pt}\item
    \begin{tabular*}{1.0\textwidth}[t]{l@{\extracolsep{\fill}}r}
      \textbf{#1} \textit{#2} & \textbf{\small #3} \\
      \textit{\small#4} & \textit{\small #5} \\
    \end{tabular*}\vspace{-7pt}
}

\newcommand{\resumePubHeading}[3]{
  \vspace{-2pt}\item
    \begin{tabular*}{1.0\textwidth}[t]{l@{\extracolsep{\fill}}r}
      \textbf{#1} & \textbf{\small{#3}} \\
      \small{#2}\\
    \end{tabular*}\\
    \vspace*{-1pt}
}

\newcommand{\resumeSubSubheading}[2]{
  \vspace{-2pt}\item
    \begin{tabular*}{1.0\textwidth}[t]{l@{\extracolsep{\fill}}r}
      \textbf{#1} & \textbf{\small{#2}}
    \end{tabular*}\\
    \vspace*{-1pt}
}

\newcommand{\resumeProjectHeading}[2]{
    \item
    \begin{tabular*}{1.001\textwidth}{l@{\extracolsep{\fill}}r}
      \small#1 & \textbf{\small #2}\\
    \end{tabular*}\vspace{-7pt}
}

\newcommand{\resumeSubItem}[1]{\resumeItem{#1}\vspace{-4pt}}

\renewcommand\labelitemi{$\vcenter{\hbox{\tiny$\bullet$}}$}
\renewcommand\labelitemii{$\vcenter{\hbox{\tiny$\bullet$}}$}

\newcommand{\resumeSubHeadingListStart}{\begin{itemize}[leftmargin=0.0in, label={}]}
\newcommand{\resumeSubHeadingListEnd}{\end{itemize}}
\newcommand{\resumeItemListStart}{\begin{itemize}}
\newcommand{\resumeItemListEnd}{\end{itemize}\vspace{-3pt}}

%-------------------------------------------
%%%%%%  RESUME STARTS HERE  %%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}


\begin{center}
    {\Large \textbf{Yuqi Xie} } \\ \vspace{1pt}
    2901 San Jacinto Blvd Apt 304, Austin, TX, 78705 \\ \vspace{1pt}
    \small \raisebox{-0.1\height}\faPhone\ 737-341-4564 ~ \href{mailto:xieleo@utexas.edu}{\raisebox{-0.2\height}\faEnvelope\  \underline{xieleo@utexas.edu}} ~ 
    %\href{https://linkedin.com/in//}{\raisebox{-0.2\height}\faLinkedin\ \underline{linkedin.com/in/username}}  ~
    \href{https://xieleo5.github.io}{\raisebox{-0.2\height}\faGithub\ \underline{xieleo5.github.io}} ~
    \href{https://scholar.google.com/citations?user=bGB6wacAAAAJ&hl=en}{\raisebox{-0.2\height}\faGraduationCap\ \underline{Google Scholar}} ~
    \vspace{-8pt}
\end{center}


%-----------EDUCATION-----------
\section{Education}
\resumeSubHeadingListStart
    \resumeSubheading
      {University of Texas at Austin}{GPA(4.0/4.0)}{Sep. 2023 -- Present}
      {Master's in Computer Science - College of Natural Science}{Texas, U.S.}
    \vspace*{4.0\multicolsep}
  \resumeSubHeadingListEnd
  \resumeSubHeadingListStart
    \resumeSubheading
      {University of Michigan, Ann Arbor}{}{Sep. 2021 -- Apr. 2023}
      {Bachelor of Computer Science - College of Engineering}{Michigan, U.S.}
    % \vspace*{4.0\multicolsep}
  \resumeSubHeadingListEnd
 
%-----------PROJECTS-----------
\section{Publication}
% \vspace{-15pt}
\resumeSubHeadingListStart
  \resumePubHeading
    {\textbf{DexMimicGen: Automated Data Generation for Bimanual Dexterous Manipulation via Imitation Learning}}
    {Zhenyu Jiang*, \textbf{Yuqi Xie}*, Kevin Lin*, Zhenjia Xu, Weikang Wan, Ajay Mandlekar$^{\dag}$, Linxi ``Jim" Fan$^{\dag}$, Yuke Zhu$^{\dag}$}
    {}
  \resumePubHeading
    {\textbf{HARMON: Whole-Body Motion Generation of Humanoid Robots from Language Descriptions}}
    {Zhenyu Jiang*, \textbf{Yuqi Xie*}, Jinhan Li, Yifeng Zhu, Ye Yuan, Yuke Zhu}
    {\href{https://openreview.net/forum?id=UUZ4Yw3lt0}{\underline{CoRL}}, 2024}

  \resumePubHeading
    {\textbf{OKAMI: Teaching Humanoid Robots Manipulation Skills through Single Video Imitation}}
    {Jinhan Li, Yifeng Zhu*, \textbf{Yuqi Xie*}, Zhenyu Jiang*, Mingyo Seo, Georgios Pavlakos, Yuke Zhu}
    {\href{https://openreview.net/forum?id=URj5TQTAXM}{\underline{CoRL}}, 2024}

  % \resumePubHeading
  %   {\textbf{Metamon: Competitive Pok√©mon as a Versatile Research Platform for Adaptive Agents}}
  %   {Jake Grigsby, \textbf{Yuqi Xie*}, Steven Zheng*, Justin Sasek*, Yuke Zhu}
  %   {In Submission}
    
  \resumePubHeading
    {\textbf{Voyager: An Open-Ended Embodied Agent with Large Language Models}}
    {Guanzhi Wang, \textbf{Yuqi Xie}, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, Anima Anandkumar}
    {\href{https://voyager.minedojo.org}{\underline{TMLR}}, 2024}

    % \vspace{-15pt}
\resumeSubHeadingListEnd
\section{Work Experience}
% \vspace{-15pt}
\resumeSubHeadingListStart
  \resumeSubheading{\textbf{NVIDIA, Generalist Embodied Agent Research (GEAR)}}{}{Jan, 2024 -- Present}{Research Intern $|$ Advised by Prof. Yuke Zhu, Dr. Jim Fan, and Dr. Ajay Mandlekar}{}
  \resumeItemListStart
  % \vspace*{-2pt}
  \resumeItem{
    \textbf{Massive Humanoid Data Generation for VLA and World-Model Training}
        
    Focused on leveraging simulation for training generalist humanoid policies by combining simulation data with real-world datasets, significantly reducing the need for costly and labor-intensive data collection.
    \vspace*{-2pt}
    \begin{itemize}[leftmargin=*]
        \item Created a \textbf{digital twin} environment, enabling real2sim2real by collecting real data and generating simulated data.
        \item Co-trained a \textbf{diffusion policy} model using both simulated and real-world data to enhance real-world policy learning.
        \item Trained a \textbf{Visual Language Action} (VLA) model, enhancing its generalization across diverse tasks using generated data.
        \item Developed a high-quality data engine in Omniverse, producing photorealistic manipulation videos with large-scale GPU clusters (L40 GPUs), which fed into the training of the neural simulation model (World Model for robotics).
    \end{itemize}
  }
  \resumeItem{
    \textbf{Real-world Humanoid Control Stack and Tele-Operation Interface}
        
    Due to the complexity of humanoid robots with dual arms and dexterous hands, traditional control systems struggle with the high number of joints. To enable more intuitive and accurate control, we focused on improving the teleoperation interface, making it easier for humans to manipulate the robot.
    \vspace*{-2pt}
    \begin{itemize}[leftmargin=*]
        \item Developed a \textbf{dexterous hand retargeting system} using a combination of inverse kinematics (IK) and angle mapping, allowing more intuitive control over different dexterous hand models.
        \item Implemented a \textbf{general body retargeting pipeline} utilizing pink IK, allowing control signals to be processed from various input devices, including the \textbf{Apple Vision Pro}, \textbf{Manus Xsense Glove}, and \textbf{Vive trackers}.
    \end{itemize}
  }
  \resumeItemListEnd
  % \small \begin{itemize}
  % \vspace*{-2pt}
  %     \item Responsible for the \href{https://www.youtube.com/watch?v=kr7FaZPFp6M}{GTC robotics demo} of \textbf{Fourier-GR1}, \textbf{Apprtronik-Apollo}, \textbf{Unitree-H1}, and \textbf{Agility-Digit} humanoids. 
  %     \item Develop a general human \textbf{tele-operation system} using \textbf{Apple-Vision-Pro} for intuitive humanoid robot control.
  %     \item 
  %     \item Responsible for collecting the \textbf{gaming and robotics video dataset} for robotics World Model training.
  %     % \item Migrate and render the RoboCasa dataset using \textbf{Isaac Lab} and use the dataset for foundation model training.
  %     \item Help to improve the \textbf{OSMO} infra to support large scale fountation model training using \textbf{thousands of H100 GPUs}.
  % \end{itemize}
\resumeSubHeadingListEnd
\vspace{-15pt}
%-----------RESEARCH----------- 
\section{Research Experience}
  \resumeSubHeadingListStart
    \resumeSubheading
    {RPL lab at UT Austin, Collaborate with NVIDIA}{}{August 2023 -- Present}
    {Student Research Assistant $|$ \small Advised by Prof. Yuke Zhu and Dr. Jim Fan}{}
    \resumeItemListStart
     \resumeItem{
        \textbf{HARMON: Whole-Body Motion Generation of Humanoid Robots from Language Descriptions}
        
        We leverage human motion priors from extensive human motion datasets to initialize humanoid motions and employ the commonsense reasoning capabilities of Vision Language Models (VLMs) to edit and refine these motions. Harmon can produce \textbf{natural, expressive, and text-aligned} humanoid motions on real humanoid robots.
        % \vspace*{-2pt}
        % \begin{itemize}[leftmargin=*]
        %     \item Employ a \textbf{diffusion-based generative model} to initialize human motions from text and retarget to humanoid robot.
        %     \item Leverage the \textbf{Vision Language Model} to edit the humanoid motion and generate finger and head motions.
        %     \item Decouple the upper- and lower-body motions using Large Language Model to realize the \textbf{whole-body control}.
        % \end{itemize}
      }
      \resumeItem{
        \textbf{OKAMI: Teaching Humanoid Robots Manipulation Skills through Single Video Imitation}
        
        We introduce OKAMI, an algorithm that generates a reference plan from a \textbf{single RGB-D video}, and derive a policy that follows the plan to complete the task. We investigate an \textbf{object-aware retargeting} approach in OKAMI.
        % \vspace*{-2pt}
        % \begin{itemize}[leftmargin=*]
        %     \item Reconstruct \textbf{SMPL 3D human} mesh from single monocular RGB video and retarget it to the humanoid robot.
        %     \item Generates feasible motions from a single human video while \textbf{adapting to new object locations} at test time.
        %     \item Use \textbf{Vision Language Model} to identify task-relevant objects without additional human inputs.
        % \end{itemize}
      }
      \resumeItem{
        \textbf{Voyager: An Open-Ended Embodied Agent with Large Language Models}
        
        Voyager is the first \textbf{lifelong learning agent} that plays Minecraft purely in context. It continuously improves itself by writing, refining, committing, and retrieving code from a skill library, all without relying on gradient descent.
        % \vspace*{-2pt}
        % \begin{itemize}[leftmargin=*]
        %     \item Designed the \textbf{curriculum}, \textbf{iterative prompting}, \textbf{self-verification}, and \textbf{skill library} for the agent.
        %     \item Proposed the \textbf{warm-up schedule} and the \textbf{auto-resume mechanism} to optimize the training process.
        %     \item Leveraged OpenAI's GPT API to generate \textbf{Code as Policy} and used \textbf{Chain-of-Thought} prompting to improve code.
        %     % \item Created the control primitives as both helper functions and examples of code writing.
        %     % \item Developed the asynchronous backend server in JavaScript for agent control and game observation.
        %     % \item Conducted experiments on downstream unseen tasks and involved human interactions for building tasks.
        % \end{itemize}
      }

    \resumeItemListEnd  
  \resumeSubHeadingListEnd
\vspace{-15pt}

%-----------PROGRAMMING SKILLS-----------
\section{Technical Skills}
  % \vspace{-5pt}
 \begin{itemize}[leftmargin=0.15in, label={}]
    \small{\item{
     \textbf{Skills}{: Robotics, Foundation Model, Prompt Engineering, Reinforcement Learning, Infrastructure Designing} \\
     \textbf{Software and Libraries}{: Pytorch, Isaac Lab, Mujoco, Unreal, Ray, Docker, React, Unity} \\
     \textbf{Programming Language}{: Python, Java, C++, JavaScript, SQL} \\
    }}
 \end{itemize}
 \vspace{-16pt}

\end{document}
